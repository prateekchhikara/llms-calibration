{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/simpleQA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>Who received the IEEE Frank Rosenblatt Award i...</td>\n",
       "      <td>Michio Sugeno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>Who was awarded the Oceanography Society's Jer...</td>\n",
       "      <td>Annick Bricaud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'topic': 'Geography', 'answer_type': 'Place',...</td>\n",
       "      <td>What's the name of the women's liberal arts co...</td>\n",
       "      <td>Radcliffe College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'topic': 'Sports', 'answer_type': 'Person', '...</td>\n",
       "      <td>In whose honor was the Leipzig 1877 tournament...</td>\n",
       "      <td>Adolf Anderssen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'topic': 'Art', 'answer_type': 'Person', 'url...</td>\n",
       "      <td>According to Karl Küchler, what did Empress El...</td>\n",
       "      <td>Poet Henrich Heine.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            metadata  \\\n",
       "0  {'topic': 'Science and technology', 'answer_ty...   \n",
       "1  {'topic': 'Science and technology', 'answer_ty...   \n",
       "2  {'topic': 'Geography', 'answer_type': 'Place',...   \n",
       "3  {'topic': 'Sports', 'answer_type': 'Person', '...   \n",
       "4  {'topic': 'Art', 'answer_type': 'Person', 'url...   \n",
       "\n",
       "                                             problem               answer  \n",
       "0  Who received the IEEE Frank Rosenblatt Award i...        Michio Sugeno  \n",
       "1  Who was awarded the Oceanography Society's Jer...       Annick Bricaud  \n",
       "2  What's the name of the women's liberal arts co...    Radcliffe College  \n",
       "3  In whose honor was the Leipzig 1877 tournament...      Adolf Anderssen  \n",
       "4  According to Karl Küchler, what did Empress El...  Poet Henrich Heine.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>{'topic': 'Sports', 'answer_type': 'Number', '...</td>\n",
       "      <td>How many gold medals did Kristin Otto win at t...</td>\n",
       "      <td>5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>{'topic': 'Video games', 'answer_type': 'Date'...</td>\n",
       "      <td>What day, month, and year did the Terraria ver...</td>\n",
       "      <td>June 2nd, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>What year was John Monteath Robertson awarded ...</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>{'topic': 'Geography', 'answer_type': 'Person'...</td>\n",
       "      <td>What is the name of the settler who selected t...</td>\n",
       "      <td>James B. Patterson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>In which year was it reported that some lichen...</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>{'topic': 'Art', 'answer_type': 'Date', 'urls'...</td>\n",
       "      <td>The book \"Rhine\" by Anselm Kiefer is from what...</td>\n",
       "      <td>1981.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>{'topic': 'Video games', 'answer_type': 'Perso...</td>\n",
       "      <td>What was the first and last name of the voice ...</td>\n",
       "      <td>Jodelle Ferland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323</th>\n",
       "      <td>{'topic': 'Music', 'answer_type': 'Date', 'url...</td>\n",
       "      <td>What month and year was Miranda Lambert's albu...</td>\n",
       "      <td>October 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>{'topic': 'Sports', 'answer_type': 'Date', 'ur...</td>\n",
       "      <td>Provide the day, month, and year Gazprom becam...</td>\n",
       "      <td>17th July 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>{'topic': 'Other', 'answer_type': 'Other', 'ur...</td>\n",
       "      <td>What instrument did Kunihiko Kodaira's father ...</td>\n",
       "      <td>A piano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               metadata  \\\n",
       "4316  {'topic': 'Sports', 'answer_type': 'Number', '...   \n",
       "4317  {'topic': 'Video games', 'answer_type': 'Date'...   \n",
       "4318  {'topic': 'Science and technology', 'answer_ty...   \n",
       "4319  {'topic': 'Geography', 'answer_type': 'Person'...   \n",
       "4320  {'topic': 'Science and technology', 'answer_ty...   \n",
       "4321  {'topic': 'Art', 'answer_type': 'Date', 'urls'...   \n",
       "4322  {'topic': 'Video games', 'answer_type': 'Perso...   \n",
       "4323  {'topic': 'Music', 'answer_type': 'Date', 'url...   \n",
       "4324  {'topic': 'Sports', 'answer_type': 'Date', 'ur...   \n",
       "4325  {'topic': 'Other', 'answer_type': 'Other', 'ur...   \n",
       "\n",
       "                                                problem              answer  \n",
       "4316  How many gold medals did Kristin Otto win at t...                  5.  \n",
       "4317  What day, month, and year did the Terraria ver...      June 2nd, 2011  \n",
       "4318  What year was John Monteath Robertson awarded ...                1983  \n",
       "4319  What is the name of the settler who selected t...  James B. Patterson  \n",
       "4320  In which year was it reported that some lichen...                2012  \n",
       "4321  The book \"Rhine\" by Anselm Kiefer is from what...               1981.  \n",
       "4322  What was the first and last name of the voice ...     Jodelle Ferland  \n",
       "4323  What month and year was Miranda Lambert's albu...        October 2010  \n",
       "4324  Provide the day, month, and year Gazprom becam...      17th July 2012  \n",
       "4325  What instrument did Kunihiko Kodaira's father ...             A piano  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt for generating distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert synthetic data generator. Your task is to generate three plausible but incorrect answers to a given question.\n",
    "\n",
    "Guidelines for generating wrong answers:\n",
    "1. Each answer should be factually incorrect but plausible within the context\n",
    "2. Match the answer type (e.g. if asking for a date, provide wrong dates)\n",
    "3. The wrong answers should be clearly distinct from the correct answer and from each other\n",
    "4. Maintain a similar level of specificity as the original answer\n",
    "5. The answers should be realistic and not obviously wrong\n",
    "\n",
    "Example 1:\n",
    "Question: What is the capital of France?\n",
    "Answer: Paris\n",
    "Wrong Answers: \n",
    "- Lyon\n",
    "- Marseille \n",
    "- Bordeaux\n",
    "Reason: All are major French cities, but incorrect as capital\n",
    "\n",
    "Example 2:\n",
    "Question: Who was the first president of the United States?\n",
    "Answer: George Washington\n",
    "Wrong Answers:\n",
    "- John Adams\n",
    "- Thomas Jefferson\n",
    "- Benjamin Franklin\n",
    "Reason: All are founding fathers but not the first president\n",
    "\n",
    "Example 3:\n",
    "Question: In what year did World War II end?\n",
    "Answer: 1945\n",
    "Wrong Answers:\n",
    "- 1943\n",
    "- 1944\n",
    "- 1946\n",
    "Reason: All are plausible years during or near WWII but not when it ended\n",
    "\n",
    "Example 4:\n",
    "Question: Who wrote Romeo and Juliet?\n",
    "Answer: William Shakespeare\n",
    "Wrong Answers:\n",
    "- Christopher Marlowe\n",
    "- Ben Jonson\n",
    "- John Webster\n",
    "Reason: All are prominent Elizabethan playwrights\n",
    "\n",
    "Example 5:\n",
    "Question: What is the largest planet in our solar system?\n",
    "Answer: Jupiter\n",
    "Wrong Answers:\n",
    "- Saturn\n",
    "- Neptune\n",
    "- Uranus\n",
    "Reason: All are gas giant planets, but smaller than Jupiter\n",
    "\n",
    "Please generate three wrong answers that follow these guidelines for the given question.\n",
    "The answers should be:\n",
    "- Factually incorrect but plausible\n",
    "- Match the same answer type (e.g. date, person, number)\n",
    "- Clearly distinct from the correct answer and each other\n",
    "- Similar in specificity/detail level\n",
    "- Realistic and not obviously wrong\n",
    "\n",
    "Return only three wrong answers as a list in JSON format with the following requirements:\n",
    "- Each wrong answer should be a string\n",
    "- The output should be a single JSON object with key \"wrong_answers\" \n",
    "- The value should be an array of exactly 3 wrong answers\n",
    "- No explanations or additional text should be included\n",
    "- The answers should maintain consistent formatting with the correct answer\n",
    "\n",
    "Example format:\n",
    "{{\n",
    "    \"wrong_answers\": [\"Wrong Answer 1\", \"Wrong Answer 2\", \"Wrong Answer 3\"]\n",
    "}}\n",
    "\n",
    "Question: {question}\n",
    "Correct Answer: {answer}\n",
    "Generate three wrong answers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM call to generate distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wrong_answer(question, answer):\n",
    "    \"\"\"\n",
    "    Generate 3 plausible but incorrect answers for a given question using GPT-4.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to generate wrong answers for\n",
    "        answer (str): The correct answer to the question\n",
    "        \n",
    "    Returns:\n",
    "        list: List of 3 wrong answers, or empty list if generation fails\n",
    "        \n",
    "    The function will retry up to 3 times if the API call fails.\n",
    "    Wrong answers are generated to be:\n",
    "    - Factually incorrect but plausible\n",
    "    - Match the same answer type as correct answer\n",
    "    - Clearly distinct from correct answer and each other\n",
    "    - Similar in specificity/detail level\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            updated_prompt = prompt.format(question=question, answer=answer)\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": updated_prompt}],\n",
    "                temperature=1,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content)['wrong_answers']\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            continue\n",
    "\n",
    "    return []\n",
    "\n",
    "print(generate_wrong_answer(\"What is the capital of India?\", \"New Delhi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_incorrect_answers = defaultdict(list)\n",
    "\n",
    "def process_row(index, df):\n",
    "    problem = df['problem'][index]\n",
    "    answer = df['answer'][index]\n",
    "    wrong_answer = generate_wrong_answer(problem, answer)\n",
    "    return index, wrong_answer\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for index, wrong_answer in tqdm(\n",
    "        executor.map(partial(process_row, df=df), range(len(df))), \n",
    "        total=len(df)\n",
    "    ):\n",
    "        index_incorrect_answers[index] = wrong_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_incorrect_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index_incorrect_answers_final.json', 'r') as f:\n",
    "    index_incorrect_answers_final = json.load(f)\n",
    "\n",
    "len(index_incorrect_answers_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new dataframe with the wrong answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrong = df.copy()\n",
    "df_wrong['wrong_answer_1'] = df_wrong.index.map(lambda x: index_incorrect_answers_final[str(x)][0])\n",
    "df_wrong['wrong_answer_2'] = df_wrong.index.map(lambda x: index_incorrect_answers_final[str(x)][1]) \n",
    "df_wrong['wrong_answer_3'] = df_wrong.index.map(lambda x: index_incorrect_answers_final[str(x)][2])\n",
    "\n",
    "df_wrong.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrong.to_csv('synthetic_dataset_with_wrong_answers.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
